{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "547b3426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os, librosa\n",
    "from tqdm import tqdm\n",
    "import torchaudio\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle \n",
    "import sklearn \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37d66a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('audio_data.pkl', 'rb') as f:\n",
    "    audio_data_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5674c24d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 126)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_data_df[49][0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b24fc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from class_CNN import DatasetClass\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "train_df, test_df = train_test_split(audio_data_df, test_size=0.2, random_state=42)\n",
    "test_df, val_df = train_test_split(test_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# call my dataclass \n",
    "\n",
    "train_data = DatasetClass(train_df)\n",
    "val_data = DatasetClass(val_df)\n",
    "test_data = DatasetClass(test_df)\n",
    "\n",
    "#save test data to pickle file\n",
    "# with open('test_data.pkl', 'wb') as f:\n",
    "#     pickle.dump(test_data, f)\n",
    "\n",
    "#call my dataloader \n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8ce1ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 127)\n",
      "torch.Size([32, 128, 127])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0][0].shape)\n",
    "for i, (X, y) in enumerate(train_loader):\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "022db1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classification(\n",
       "  (cnn): CNN1D(\n",
       "    (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (gelu): GELU(approximate='none')\n",
       "  )\n",
       "  (encoder_blocks): ModuleList(\n",
       "    (0): EncoderBlock(\n",
       "      (ln1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (ffn): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=256, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import model\n",
    "\n",
    "from model_encoder import classification, CNN1D\n",
    "import torch\n",
    "\n",
    "model = classification()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48307a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/week5/Encoder/wandb/run-20250514_140657-hzquigio</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lilounina-nina-the-machine-learning-institute/week5/runs/hzquigio' target=\"_blank\">flick-20250514_1406_57</a></strong> to <a href='https://wandb.ai/lilounina-nina-the-machine-learning-institute/week5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lilounina-nina-the-machine-learning-institute/week5' target=\"_blank\">https://wandb.ai/lilounina-nina-the-machine-learning-institute/week5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lilounina-nina-the-machine-learning-institute/week5/runs/hzquigio' target=\"_blank\">https://wandb.ai/lilounina-nina-the-machine-learning-institute/week5/runs/hzquigio</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/lilounina-nina-the-machine-learning-institute/week5/runs/hzquigio?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x72a880247c70>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb \n",
    "import datetime\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M_%S')\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "wandb.init(\n",
    "    project=\"week5\",\n",
    "    name=f\"flick-{timestamp}\",\n",
    "    config={\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"learning_rate\": 0.001\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48307a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.9943, Val Loss: 1.7417\n",
      "Epoch 2, Train Loss: 1.6360, Val Loss: 1.6966\n",
      "Epoch 3, Train Loss: 1.4384, Val Loss: 1.5336\n",
      "Epoch 4, Train Loss: 1.3043, Val Loss: 1.3220\n",
      "Epoch 5, Train Loss: 1.1955, Val Loss: 1.3559\n",
      "Epoch 6, Train Loss: 1.1367, Val Loss: 1.1085\n",
      "Epoch 7, Train Loss: 1.0417, Val Loss: 1.2066\n",
      "Epoch 8, Train Loss: 1.0134, Val Loss: 1.1837\n",
      "Epoch 9, Train Loss: 0.9100, Val Loss: 0.9364\n",
      "Epoch 10, Train Loss: 0.8808, Val Loss: 0.9345\n",
      "Epoch 11, Train Loss: 0.8371, Val Loss: 0.8601\n",
      "Epoch 12, Train Loss: 0.8146, Val Loss: 0.8964\n",
      "Epoch 13, Train Loss: 0.7433, Val Loss: 0.8391\n",
      "Epoch 14, Train Loss: 0.7270, Val Loss: 0.8394\n",
      "Epoch 15, Train Loss: 0.6827, Val Loss: 0.7515\n",
      "Epoch 16, Train Loss: 0.6398, Val Loss: 0.6834\n",
      "Epoch 17, Train Loss: 0.6254, Val Loss: 0.7696\n",
      "Epoch 18, Train Loss: 0.5836, Val Loss: 0.6141\n",
      "Epoch 19, Train Loss: 0.5584, Val Loss: 0.6410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Train Loss: 0.5099, Val Loss: 0.6662\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>█▆▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>val_loss</td><td>██▇▅▆▄▅▅▃▃▃▃▂▂▂▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>0.50987</td></tr><tr><td>val_loss</td><td>0.66617</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">flick-20250514_1406_57</strong> at: <a href='https://wandb.ai/lilounina-nina-the-machine-learning-institute/week5/runs/hzquigio' target=\"_blank\">https://wandb.ai/lilounina-nina-the-machine-learning-institute/week5/runs/hzquigio</a><br> View project at: <a href='https://wandb.ai/lilounina-nina-the-machine-learning-institute/week5' target=\"_blank\">https://wandb.ai/lilounina-nina-the-machine-learning-institute/week5</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250514_140657-hzquigio/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for padded_spec, label in train_loader:\n",
    "        #print(padded_spec.shape)\n",
    "        padded_spec = padded_spec.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        output = model(padded_spec)\n",
    "\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "  \n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for padded_spec, label in val_loader:\n",
    "            padded_spec = padded_spec.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            output = model(padded_spec)\n",
    "            loss = criterion(output, label)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    wandb.log({\"train_loss\": avg_train_loss, \"val_loss\": avg_val_loss})\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "wandb.finish()\n",
    "    # print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d724db8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"encoder.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
